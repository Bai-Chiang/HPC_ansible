# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ClusterName=example
SlurmctldHost=node1
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity,task/cgroup
#
#
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#
#
# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
#SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log
#
#

GresTypes=gpu
RebootProgram="/usr/bin/systemctl reboot"
ResumeTimeout=120


# COMPUTE NODES
# To get total memory run `slurm -C` or `free --mebi`
NodeName=node[10-11] NodeAddr=192.168.122.[10-11] Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 ealMemory=1024 State=UNKNOWN Gres=gpu:1 Weight=1
NodeName=node20 NodeAddr=192.168.122.20 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=1024 State=UNKNOWN Gres=gpu:1 Weight=10
PartitionName=main Nodes=node[10-11],node20 Default=YES DefaultTime=03-00:00:00 MaxTime=INFINITE State=UP

