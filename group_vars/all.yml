---
# Only admins could login compute nodes
admins: 'tux admin1'

# List of NFS mount points
nfs_share_list: 
  - { name: home, server_ip: 192.168.122.1, server_dir: /home, client_mount_point: /home, mount_opts: 'defaults,_netdev', owner: root, group: root, mode: '0755' }
  - { name: data, server_ip: 192.168.122.1, server_dir: /data, client_mount_point: /data, mount_opts: 'defaults,_netdev,nofail', owner: root, group: root, mode: '0755' }

# SLURM configs
munge_key_file: files/slurm/munge.key
slurm_config_file: files/slurm/slurm.conf
slurm_cgroup_config_file: files/slurm/cgroup.conf
slurm_gres_config_file: files/slurm/gres.conf

# slurm daemon's nice value (nice value on compute nodes)
slurmd_nice: 10

# The ip/mask of the cluster
slurm_cluster_ip: 192.168.122.0/24

# The control node ip
slurm_control_node_ip: 192.168.122.1

# all nodes and its ip address, appended to /etc/hosts file
slurm_nodes_hosts:
  - { hostname: node1,  ip: 192.168.122.1 }
  - { hostname: node10, ip: 192.168.122.10 }
  - { hostname: node11, ip: 192.168.122.11 }
  - { hostname: node20, ip: 192.168.122.20 }

# Encrypt this file using for example ansible vault to prevent leak user info.
slurm_users:
  - name: johnsmith
    uid: 1234
    state: present
    password: $6$mysecretsalt$ZB9R8AirQYAXhtfhOo2qdJz52FyNI6v3L6Uc3KNRP.arBKIYpcuEyQewT5qBAHoyQFwHkW6Z551Ql.cZ53GeY0
    addition_groups: secret_group,another_group
    ssh_public_key: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIB3uVHbEg3FnFodk/2fpDD/U1Ousnrqke8i2WP855zVN

slurm_groups:
  - { name: secret_group, gid: 12345, state: present }
  - { name: another_group, gid: 23456, state: present }
